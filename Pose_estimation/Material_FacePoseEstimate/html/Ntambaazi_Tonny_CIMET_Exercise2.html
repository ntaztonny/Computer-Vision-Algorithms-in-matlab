
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 2 report, title "face pose estimation", Ntambaazi Tonny_CIMET : ...,</title><meta name="generator" content="MATLAB 9.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-04-11"><meta name="DC.source" content="Ntambaazi_Tonny_CIMET_Exercise2.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 2 report, title "face pose estimation", Ntambaazi Tonny_CIMET : ...,</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Part I.  Synthesis of a sequence of images</a></li><li><a href="#2">1/ Load Data (Vertices, Faces, Texture), Display and rotate</a></li><li><a href="#3">camera location on the optical axis (OZ)</a></li><li><a href="#4">1_ 2</a></li><li><a href="#6">Part 2_1 Feature points detection and matching</a></li><li><a href="#7">Part 2_2 The Ransac algorithm</a></li><li><a href="#8">Part 2_2 The Ransac algorithm: testing larger angles</a></li><li><a href="#9">Part 3_1 Computation of the fundamental matrix</a></li><li><a href="#10">Part 3_2 Computation of the essential matrix</a></li></ul></div><h2>Part I.  Synthesis of a sequence of images<a name="1"></a></h2><pre class="codeinput">clear; close <span class="string">all</span>; clc
</pre><h2>1/ Load Data (Vertices, Faces, Texture), Display and rotate<a name="2"></a></h2><pre class="codeinput">load(<span class="string">'model_3D_01.mat'</span>); <span class="comment">% faces (Nfx3); vertices(Nx3); texture(Nx3);</span>
<span class="comment">% change z direction</span>
vertices(:,3)=-vertices(:,3);
<span class="comment">% scale so that the face is in a bounding box [-1 1,-1 1,-1 1]</span>
vertices=vertices/max(vertices(:));

<span class="comment">% Size of the image</span>
Nx=400;Ny=Nx;
<span class="comment">% size of the pixel pitch and focal lenght</span>
pix=20e-3; <span class="comment">% in mm</span>
focal=50; <span class="comment">% in mm</span>
<span class="comment">% field of view computation</span>
fov=2*atand(Ny*pix/2/focal);<span class="comment">% Field of view of the camera in degrees</span>
</pre><h2>camera location on the optical axis (OZ)<a name="3"></a></h2><pre class="codeinput">poscamZ = -20;

phi=0:10:90;<span class="comment">% rotation angles</span>
<span class="keyword">for</span> c=1:size(phi,2)
    <span class="comment">%1_3</span>
    <span class="comment">%setting the rotation to the Y 90%</span>
    RY=XYZrotation(phi(c),2);
    verticesR=(RY*vertices')';
    display_face_fac_vert_tex0(faces, verticesR,texture, [Nx Ny],poscamZ);
    <span class="comment">% to keep the same field of view  given by the first display</span>
    <span class="comment">%     if b==0, ax=gca;fov=ax.CameraViewAngle;b=1;</span>
    camva(fov);  <span class="comment">% Set the camera field of view (see help Camera Graphics Terminology)</span>
    <span class="comment">% to impose a field of view  given by the pixel size and the image size</span>
    <span class="comment">% camva(fov);  % Set the camera field of view</span>
    pause(0.3)
    <span class="comment">% Grab the rendered frame</span>
    F = getframe(gcf);
    I(c).image=F.cdata;
    <span class="comment">% save of the image</span>
    imwrite(I(c).image,sprintf(<span class="string">'model_%03d.png'</span>,phi(c)))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Ntambaazi_Tonny_CIMET_Exercise2_01.png" style="width:400px;height:400px;" alt=""> <h2>1_ 2<a name="4"></a></h2><pre class="codeinput"> dbtype(<span class="string">'display_face_fac_vert_tex0.m'</span>)
 <span class="comment">% The display_face_fac_vert_tex0.m function  takes parameters of the</span>
 <span class="comment">% faces, vertices, texture, fpos, poscamZ:</span>
 <span class="comment">%1:creates a render using the opengl API, and also creates the window displace plane: 2: using the mesh_h = trimesh(); it generates the facial mesh, using the phong lighting model:</span>
 <span class="comment">%Using the set matlab function, with parameters of the aspect ratio,</span>
 <span class="comment">%position, perspective projection, camera position,</span>
 <span class="comment">% Finally, the color, and material and light color... are all set by this</span>
 <span class="comment">% function</span>
</pre><pre class="codeoutput">
1     function display_face_fac_vert_tex0(faces, vertices,texture, fpos,poscamZ)
2     % Use : display_face_fac_vert_tex(faces, vertices,texture, fpos,poscamZ)
3     % Inputs :
4     % *faces : gives the 3 numbering of the vertices that create faces (Nfx3)
5     % *vertices : XYZ of each vertices (Nvx3)
6     % *texture : RGB of each vertices (Nvx3)
7     % *fpos : raw vector , fpos(1) is the width of the displayed window , fpos(2) is the height of the displayed window
8     % *poscamZ : camera position (&lt;0)
9     
10    set(gcf, 'Renderer', 'opengl');
11    fig_pos = get(gcf, 'Position');
12    fig_pos(3) = fpos(1);%rp.width
13    fig_pos(4) = fpos(2);%rp.height
14    set(gcf, 'Position', fig_pos);
15    set(gcf, 'Resize', 'off');
16    
17    mesh_h = trimesh(...
18        faces, vertices(:, 1), vertices(:, 2), vertices(:, 3), ...
19        'EdgeColor', 'none', ...
20        'FaceVertexCData', texture/255, 'FaceColor', 'interp', ...
21        'FaceLighting', 'phong' ...
22        );
23    
24    
25    set(gca,'DataAspectRatio',[ 1 1 1 ],'Units','pixels','Position',[ 0 0 fig_pos(3) fig_pos(4) ],...
26          'Visible','off','Box','off','Projection','perspective','CameraPosition',[0 0 poscamZ],...
27          'CameraTarget',[0 0 1],'CameraUpVector',[0 1 0]);
28    % ax = gca;
29    % ax.DataAspectRatio=[ 1 1 1 ];%     'PlotBoxAspectRatio', [ 1 1 1 ], ...
30    % ax.Units='pixels';
31    % ax.Position=[ 0 0 fig_pos(3) fig_pos(4) ];
32    % ax.Visible='off';
33    % ax.Box='off';
34    % ax.Projection='perspective';
35    % ax.CameraPosition=[0 0 poscamZ];
36    % ax.CameraTarget=[0 0 1];
37    % ax.CameraUpVector=[0 1 0];
38    xlabel('x');ylabel('y');zlabel('z');
39    
40    fx=gcf;
41    fx.Color=[ 0 0 0 ];
42    
43    set(gcf,'Color',[ 0 0 0 ])
44    % material([.5, .5, .1 1  ])
45    % camlight('headlight');
46    
</pre><pre>Calutating the matrices
Rotation of the first image at 0 rotation:</pre><pre class="codeinput"> Rot_1 = XYZrotation(0,2);
 <span class="comment">% Rotation of the third image at 20 degree rotation</span>
 Rot_2 = XYZrotation(20,2);
 <span class="comment">% translation matrix</span>
 trans_1 = [0,0, -poscamZ];
 <span class="comment">%combining the translation and rotation matrices forming the rigid body</span>
 <span class="comment">%transform</span>
 Rot_trans1 = [Rot_1, trans_1'; 0,0,0,1];
 Rot_trans2 = [Rot_2, trans_1'; 0,0,0,1];
 <span class="comment">%K matrix</span>
 K_mat = [-1/pix, 0, Nx/2; 0, -1/pix, Nx/2; 0, 0, 1];
 <span class="comment">%Projection matrix</span>
 Pro_mat = [focal, 0, 0, 0; 0, focal, 0, 0; 0, 0, 1, 0;];
 <span class="comment">%</span>
 Calib_1 = K_mat * Pro_mat * Rot_trans1;
 Calib_2 = K_mat * Pro_mat * Rot_trans2;
 point = [vertices(3897,:), 1];

 point_1 = Calib_1 * point';
 point_2 = Calib_2 * point';
 <span class="comment">% plotting the point</span>

 point_1 = point_1 / point_1(3);
 point_2 = point_2 / point_2(3);
 I_1 = imread(<span class="string">'model_000.png'</span>);
 I_2 = imread(<span class="string">'model_020.png'</span>);

 <span class="comment">%Projection of the points using the calibration matrix on the 2 images</span>
 figure;

 subplot (1,2,1),
 imshow(I_1);
 title(<span class="string">'Point Projection on the 2 images'</span>);
 hold <span class="string">on</span>;
 plot(point_1(1,1),point_1(2,1),<span class="string">'g.'</span>);
 subplot (1,2,2), imshow(I_2);
 hold <span class="string">on</span>;
 plot(point_2(1,1),point_2(2,1),<span class="string">'g.'</span>);
</pre><img vspace="5" hspace="5" src="Ntambaazi_Tonny_CIMET_Exercise2_02.png" style="width:560px;height:420px;" alt=""> <h2>Part 2_1 Feature points detection and matching<a name="6"></a></h2><pre class="codeinput"> <span class="comment">%Starting the VI_feat_toolbox</span>
 run(<span class="string">'vlfeat/toolbox/vl_setup'</span>)

 <span class="comment">% converting the images to gray from rgb</span>
 I_gray1 = single(rgb2gray(I_1));
 I_gray2 = single(rgb2gray(I_2));<span class="comment">%single typle is recommended in the documentation</span>

 <span class="comment">% Compute the shift keypoints</span>

 [F1, D1] = vl_sift(I_gray1);
 [F2, D2] = vl_sift(I_gray2);


 <span class="comment">%  implement the basic matching algorithm.</span>
<span class="comment">%  [matches, scores] = vl_ubcmatch(D1, D2) ;</span>

<span class="comment">% Here, a threshold of 1.6 is used to minimize the number of false matches: by comparing the eucledian distance between the match points identified:</span>
<span class="comment">%if it is greater than or equal to 1.6 the point is used otherwise, it is omitted</span>

[matches, scores] = vl_ubcmatch(D1, D2, 1.6) ;
 <span class="comment">% Plot the points onto the images</span>
 <span class="comment">%image 1</span>
 figure;

 subplot(1,2,1);
 imshow(uint8(I_1));
 title(<span class="string">'Points matched in 2 images'</span>);
 hold <span class="string">on</span>;
 plot(F1(1,matches(1,:)),F1(2,matches(1,:)),<span class="string">'g.'</span>);
 <span class="comment">%Image 2</span>
 subplot(1,2,2);
 imshow(uint8(I_2));
 hold <span class="string">on</span>;
 plot(F2(1,matches(2,:)),F2(2,matches(2,:)),<span class="string">'g.'</span>);

 <span class="comment">% Comment</span>
 <span class="comment">% The displays show that the not all points are perfectly matched. A clear</span>
 <span class="comment">% example are the points around the neck. The left picture has 4 points</span>
 <span class="comment">% but te right has 3points, however, when the rdges are sharp and distinct</span>
 <span class="comment">% forexample around the eyes, there will be good matching. 4 points are</span>
 <span class="comment">% observed in both eyes of both images</span>
</pre><img vspace="5" hspace="5" src="Ntambaazi_Tonny_CIMET_Exercise2_03.png" style="width:560px;height:420px;" alt=""> <h2>Part 2_2 The Ransac algorithm<a name="7"></a></h2><pre class="codeinput">numMatches = size(matches,2);
X1 = F1(1:2,matches(1,:)) ; X1(3,:) = 1 ;
X2 = F2(1:2,matches(2,:)) ; X2(3,:) = 1 ;

<span class="keyword">for</span> t = 1:100
  <span class="comment">% estimate homograpyh</span>
  subset = vl_colsubset(1:numMatches, 4) ;
  A = [] ;
  <span class="keyword">for</span> i = subset
    A = cat(1, A, kron(X1(:,i)', vl_hat(X2(:,i)))) ;
  <span class="keyword">end</span>
  [U,S,V] = svd(A) ;
  H{t} = reshape(V(:,9),3,3) ;

  <span class="comment">% score homography</span>
  X2_ = H{t} * X1 ;
  du = X2_(1,:)./X2_(3,:) - X2(1,:)./X2(3,:) ;
  dv = X2_(2,:)./X2_(3,:) - X2(2,:)./X2(3,:) ;
  ok{t} = (du.*du + dv.*dv) &lt; 6*6 ;
  score(t) = sum(ok{t}) ;
<span class="keyword">end</span>

[score, best] = max(score) ;
H = H{best} ;
ok = ok{best} ;

<span class="comment">%show matches</span>
dh1 = max(size(I_2,1)-size(I_1,1),0) ;
dh2 = max(size(I_1,1)-size(I_2,1),0) ;


figure ; clf ;
subplot(2,1,1) ;
imagesc([padarray(I_1,dh1,<span class="string">'post'</span>) padarray(I_2,dh2,<span class="string">'post'</span>)]) ;
o = size(I_1,2) ;
line([F1(1,matches(1,:));F2(1,matches(2,:))+o], <span class="keyword">...</span>
     [F1(2,matches(1,:));F2(2,matches(2,:))]) ;
title(sprintf(<span class="string">'%d tentative matches'</span>, numMatches)) ;
axis <span class="string">image</span> <span class="string">off</span> ;

subplot(2,1,2) ;
imagesc([padarray(I_1,dh1,<span class="string">'post'</span>) padarray(I_2,dh2,<span class="string">'post'</span>)]) ;
o = size(I_1,2) ;
line([F1(1,matches(1,ok));F2(1,matches(2,ok))+o], <span class="keyword">...</span>
     [F1(2,matches(1,ok));F2(2,matches(2,ok))]) ;
title(sprintf(<span class="string">'%d (%.2f%%) inliner matches out of %d'</span>, <span class="keyword">...</span>
              sum(ok), <span class="keyword">...</span>
              100*sum(ok)/numMatches, <span class="keyword">...</span>
              numMatches)) ;
axis <span class="string">image</span> <span class="string">off</span> ;
<span class="comment">% The homography model isn't really effective for large angles: The</span>
<span class="comment">% matching is better whent there is a smaller rotation between the 2 images</span>
<span class="comment">% being matched. The alternative method is:</span>
</pre><img vspace="5" hspace="5" src="Ntambaazi_Tonny_CIMET_Exercise2_04.png" style="width:560px;height:420px;" alt=""> <h2>Part 2_2 The Ransac algorithm: testing larger angles<a name="8"></a></h2><pre class="codeinput"><span class="comment">%  Loading another sample image to test for homolograph basing on angles</span>
<span class="comment">%  I_3 = imread('model_050.png');</span>
<span class="comment">%  I_gray3 = single(rgb2gray(I_3));</span>
<span class="comment">%  [F2, D2] = vl_sift(I_gray3);</span>
<span class="comment">%</span>
<span class="comment">%  numMatches = size(matches,2);</span>
<span class="comment">% X1 = F1(1:2,matches(1,:)) ; X1(3,:) = 1 ;</span>
<span class="comment">% X2 = F2(1:2,matches(2,:)) ; X2(3,:) = 1 ;</span>
<span class="comment">%</span>
<span class="comment">% for t = 1:100</span>
<span class="comment">%   % estimate homograpyh</span>
<span class="comment">%   subset = vl_colsubset(1:numMatches, 4) ;</span>
<span class="comment">%   A = [] ;</span>
<span class="comment">%   for i = subset</span>
<span class="comment">%     A = cat(1, A, kron(X1(:,i)', vl_hat(X2(:,i)))) ;</span>
<span class="comment">%   end</span>
<span class="comment">%   [U,S,V] = svd(A) ;</span>
<span class="comment">%   H{t} = reshape(V(:,9),3,3) ;</span>
<span class="comment">%</span>
<span class="comment">%   % score homography</span>
<span class="comment">%   X2_ = H{t} * X1 ;</span>
<span class="comment">%   du = X2_(1,:)./X2_(3,:) - X2(1,:)./X2(3,:) ;</span>
<span class="comment">%   dv = X2_(2,:)./X2_(3,:) - X2(2,:)./X2(3,:) ;</span>
<span class="comment">%   ok{t} = (du.*du + dv.*dv) &lt; 6*6 ;</span>
<span class="comment">%   score(t) = sum(ok{t}) ;</span>
<span class="comment">% end</span>
<span class="comment">%</span>
<span class="comment">% [score, best] = max(score) ;</span>
<span class="comment">% H = H{best} ;</span>
<span class="comment">% ok = ok{best} ;</span>
<span class="comment">%</span>
<span class="comment">% %show matches</span>
<span class="comment">% dh1 = max(size(I_2,1)-size(I_1,1),0) ;</span>
<span class="comment">% dh2 = max(size(I_1,1)-size(I_2,1),0) ;</span>
<span class="comment">%</span>
<span class="comment">%</span>
<span class="comment">% figure(5); clf ;</span>
<span class="comment">% subplot(2,1,1) ;</span>
<span class="comment">% imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;</span>
<span class="comment">% o = size(I_1,2) ;</span>
<span class="comment">% line([F1(1,matches(1,:));F2(1,matches(2,:))+o], ...</span>
<span class="comment">%      [F1(2,matches(1,:));F2(2,matches(2,:))]) ;</span>
<span class="comment">% title(sprintf('%d tentative matches', numMatches)) ;</span>
<span class="comment">% axis image off ;</span>
<span class="comment">%</span>
<span class="comment">% subplot(2,1,2) ;</span>
<span class="comment">% imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;</span>
<span class="comment">% o = size(I_1,2) ;</span>
<span class="comment">% line([F1(1,matches(1,ok));F2(1,matches(2,ok))+o], ...</span>
<span class="comment">%      [F1(2,matches(1,ok));F2(2,matches(2,ok))]) ;</span>
<span class="comment">% title(sprintf('%d (%.2f%%) inliner matches out of %d', ...</span>
<span class="comment">%               sum(ok), ...</span>
<span class="comment">%               100*sum(ok)/numMatches, ...</span>
<span class="comment">%               numMatches)) ;</span>
<span class="comment">% axis image off ;</span>
</pre><h2>Part 3_1 Computation of the fundamental matrix<a name="9"></a></h2><pre class="codeinput">mat1 = ones( 1, size(matches,2));
pt_1 = [F1( 1:2, matches(1,:)); mat1];
pt_2 = [F2( 1:2, matches(2,:)); mat1 ];
<span class="comment">%the Fundamental matrix</span>
Fundamental_max = det_F_normalized_8point(pt_1,pt_2);

Fundamental_max

<span class="comment">%Drawing the epipolar lines in first image</span>

point_set = F1( 1:2, matches(1,:));
epiLines = epipolarLine(Fundamental_max', point_set');
<span class="comment">%Compute the intersection points of the lines and the image border.</span>
points = lineToBorderPoints(epiLines,size(I_1));

<span class="comment">%plotting points on image</span>
figure;
imshow (I_1);
title(<span class="string">'Epipolar lines on the first image'</span>)
hold <span class="string">on</span>;
line(points(:,[1,3])',points(:,[2,4])');
</pre><pre class="codeoutput">
Fundamental_max =

   -0.0001   -0.0001    0.0339
    0.0000    0.0000   -0.0127
   -0.0025    0.0168   -5.0062

</pre><img vspace="5" hspace="5" src="Ntambaazi_Tonny_CIMET_Exercise2_05.png" style="width:549px;height:490px;" alt=""> <h2>Part 3_2 Computation of the essential matrix<a name="10"></a></h2><pre class="codeinput">Essential_mat = K_mat' * Fundamental_max * K_mat;

Essential_mat
</pre><pre class="codeoutput">
Essential_mat =

   -0.1439   -0.1367   -0.5716
    0.0955    0.0180    0.1802
    0.3164   -0.3661   -0.5694

</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 2 report, title "face pose estimation", Ntambaazi Tonny_CIMET : ...,


%% Part I.  Synthesis of a sequence of images
clear; close all; clc

%% 1/ Load Data (Vertices, Faces, Texture), Display and rotate
load('model_3D_01.mat'); % faces (Nfx3); vertices(Nx3); texture(Nx3);
% change z direction
vertices(:,3)=-vertices(:,3);
% scale so that the face is in a bounding box [-1 1,-1 1,-1 1]
vertices=vertices/max(vertices(:));

% Size of the image
Nx=400;Ny=Nx;
% size of the pixel pitch and focal lenght
pix=20e-3; % in mm
focal=50; % in mm
% field of view computation
fov=2*atand(Ny*pix/2/focal);% Field of view of the camera in degrees

%% camera location on the optical axis (OZ)
poscamZ = -20;

phi=0:10:90;% rotation angles
for c=1:size(phi,2)
    %1_3
    %setting the rotation to the Y 90%
    RY=XYZrotation(phi(c),2);
    verticesR=(RY*vertices')';
    display_face_fac_vert_tex0(faces, verticesR,texture, [Nx Ny],poscamZ);
    % to keep the same field of view  given by the first display
    %     if b==0, ax=gca;fov=ax.CameraViewAngle;b=1;
    camva(fov);  % Set the camera field of view (see help Camera Graphics Terminology)
    % to impose a field of view  given by the pixel size and the image size
    % camva(fov);  % Set the camera field of view
    pause(0.3)
    % Grab the rendered frame
    F = getframe(gcf);
    I(c).image=F.cdata;
    % save of the image
    imwrite(I(c).image,sprintf('model_%03d.png',phi(c)))
end

%% 1_ 2
 dbtype('display_face_fac_vert_tex0.m')
 % The display_face_fac_vert_tex0.m function  takes parameters of the
 % faces, vertices, texture, fpos, poscamZ: 
 %1:creates a render using the opengl API, and also creates the window displace plane: 2: using the mesh_h = trimesh(); it generates the facial mesh, using the phong lighting model:
 %Using the set matlab function, with parameters of the aspect ratio,
 %position, perspective projection, camera position, 
 % Finally, the color, and material and light color... are all set by this
 % function
 %%
%  Calutating the matrices
% Rotation of the first image at 0 rotation:
 Rot_1 = XYZrotation(0,2);
 % Rotation of the third image at 20 degree rotation
 Rot_2 = XYZrotation(20,2);
 % translation matrix
 trans_1 = [0,0, -poscamZ];
 %combining the translation and rotation matrices forming the rigid body
 %transform
 Rot_trans1 = [Rot_1, trans_1'; 0,0,0,1];
 Rot_trans2 = [Rot_2, trans_1'; 0,0,0,1];
 %K matrix
 K_mat = [-1/pix, 0, Nx/2; 0, -1/pix, Nx/2; 0, 0, 1];
 %Projection matrix
 Pro_mat = [focal, 0, 0, 0; 0, focal, 0, 0; 0, 0, 1, 0;];
 %
 Calib_1 = K_mat * Pro_mat * Rot_trans1;
 Calib_2 = K_mat * Pro_mat * Rot_trans2;
 point = [vertices(3897,:), 1];
 
 point_1 = Calib_1 * point';
 point_2 = Calib_2 * point';
 % plotting the point
 
 point_1 = point_1 / point_1(3);
 point_2 = point_2 / point_2(3);
 I_1 = imread('model_000.png');
 I_2 = imread('model_020.png');
 
 %Projection of the points using the calibration matrix on the 2 images
 figure; 
 
 subplot (1,2,1), 
 imshow(I_1); 
 title('Point Projection on the 2 images');
 hold on;
 plot(point_1(1,1),point_1(2,1),'g.');
 subplot (1,2,2), imshow(I_2); 
 hold on; 
 plot(point_2(1,1),point_2(2,1),'g.');
 
 
 %% Part 2_1 Feature points detection and matching

 %Starting the VI_feat_toolbox
 run('vlfeat/toolbox/vl_setup')

 % converting the images to gray from rgb
 I_gray1 = single(rgb2gray(I_1)); 
 I_gray2 = single(rgb2gray(I_2));%single typle is recommended in the documentation
 
 % Compute the shift keypoints
 
 [F1, D1] = vl_sift(I_gray1);
 [F2, D2] = vl_sift(I_gray2);
 
 
 %  implement the basic matching algorithm. 
%  [matches, scores] = vl_ubcmatch(D1, D2) ;

% Here, a threshold of 1.6 is used to minimize the number of false matches: by comparing the eucledian distance between the match points identified: 
%if it is greater than or equal to 1.6 the point is used otherwise, it is omitted 

[matches, scores] = vl_ubcmatch(D1, D2, 1.6) ;
 % Plot the points onto the images
 %image 1
 figure;
 
 subplot(1,2,1);
 imshow(uint8(I_1));
 title('Points matched in 2 images');
 hold on;
 plot(F1(1,matches(1,:)),F1(2,matches(1,:)),'g.');
 %Image 2
 subplot(1,2,2);
 imshow(uint8(I_2));
 hold on;
 plot(F2(1,matches(2,:)),F2(2,matches(2,:)),'g.');
 
 % Comment
 % The displays show that the not all points are perfectly matched. A clear
 % example are the points around the neck. The left picture has 4 points
 % but te right has 3points, however, when the rdges are sharp and distinct
 % forexample around the eyes, there will be good matching. 4 points are
 % observed in both eyes of both images
 
 %% Part 2_2 The Ransac algorithm
 
 
numMatches = size(matches,2);
X1 = F1(1:2,matches(1,:)) ; X1(3,:) = 1 ;
X2 = F2(1:2,matches(2,:)) ; X2(3,:) = 1 ;

for t = 1:100
  % estimate homograpyh
  subset = vl_colsubset(1:numMatches, 4) ;
  A = [] ;
  for i = subset
    A = cat(1, A, kron(X1(:,i)', vl_hat(X2(:,i)))) ;
  end
  [U,S,V] = svd(A) ;
  H{t} = reshape(V(:,9),3,3) ;

  % score homography
  X2_ = H{t} * X1 ;
  du = X2_(1,:)./X2_(3,:) - X2(1,:)./X2(3,:) ;
  dv = X2_(2,:)./X2_(3,:) - X2(2,:)./X2(3,:) ;
  ok{t} = (du.*du + dv.*dv) < 6*6 ;
  score(t) = sum(ok{t}) ;
end

[score, best] = max(score) ;
H = H{best} ;
ok = ok{best} ;
 
%show matches
dh1 = max(size(I_2,1)-size(I_1,1),0) ;
dh2 = max(size(I_1,1)-size(I_2,1),0) ;


figure ; clf ;
subplot(2,1,1) ;
imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;
o = size(I_1,2) ;
line([F1(1,matches(1,:));F2(1,matches(2,:))+o], ...
     [F1(2,matches(1,:));F2(2,matches(2,:))]) ;
title(sprintf('%d tentative matches', numMatches)) ;
axis image off ;

subplot(2,1,2) ;
imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;
o = size(I_1,2) ;
line([F1(1,matches(1,ok));F2(1,matches(2,ok))+o], ...
     [F1(2,matches(1,ok));F2(2,matches(2,ok))]) ;
title(sprintf('%d (%.2f%%) inliner matches out of %d', ...
              sum(ok), ...
              100*sum(ok)/numMatches, ...
              numMatches)) ;
axis image off ;
% The homography model isn't really effective for large angles: The
% matching is better whent there is a smaller rotation between the 2 images
% being matched. The alternative method is: 
 
 %% Part 2_2 The Ransac algorithm: testing larger angles
 
%  Loading another sample image to test for homolograph basing on angles
%  I_3 = imread('model_050.png');
%  I_gray3 = single(rgb2gray(I_3));
%  [F2, D2] = vl_sift(I_gray3);
%  
%  numMatches = size(matches,2);
% X1 = F1(1:2,matches(1,:)) ; X1(3,:) = 1 ;
% X2 = F2(1:2,matches(2,:)) ; X2(3,:) = 1 ;
% 
% for t = 1:100
%   % estimate homograpyh
%   subset = vl_colsubset(1:numMatches, 4) ;
%   A = [] ;
%   for i = subset
%     A = cat(1, A, kron(X1(:,i)', vl_hat(X2(:,i)))) ;
%   end
%   [U,S,V] = svd(A) ;
%   H{t} = reshape(V(:,9),3,3) ;
% 
%   % score homography
%   X2_ = H{t} * X1 ;
%   du = X2_(1,:)./X2_(3,:) - X2(1,:)./X2(3,:) ;
%   dv = X2_(2,:)./X2_(3,:) - X2(2,:)./X2(3,:) ;
%   ok{t} = (du.*du + dv.*dv) < 6*6 ;
%   score(t) = sum(ok{t}) ;
% end
% 
% [score, best] = max(score) ;
% H = H{best} ;
% ok = ok{best} ;
%  
% %show matches
% dh1 = max(size(I_2,1)-size(I_1,1),0) ;
% dh2 = max(size(I_1,1)-size(I_2,1),0) ;
% 
% 
% figure(5); clf ;
% subplot(2,1,1) ;
% imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;
% o = size(I_1,2) ;
% line([F1(1,matches(1,:));F2(1,matches(2,:))+o], ...
%      [F1(2,matches(1,:));F2(2,matches(2,:))]) ;
% title(sprintf('%d tentative matches', numMatches)) ;
% axis image off ;
% 
% subplot(2,1,2) ;
% imagesc([padarray(I_1,dh1,'post') padarray(I_2,dh2,'post')]) ;
% o = size(I_1,2) ;
% line([F1(1,matches(1,ok));F2(1,matches(2,ok))+o], ...
%      [F1(2,matches(1,ok));F2(2,matches(2,ok))]) ;
% title(sprintf('%d (%.2f%%) inliner matches out of %d', ...
%               sum(ok), ...
%               100*sum(ok)/numMatches, ...
%               numMatches)) ;
% axis image off ;


%% Part 3_1 Computation of the fundamental matrix

mat1 = ones( 1, size(matches,2));
pt_1 = [F1( 1:2, matches(1,:)); mat1];
pt_2 = [F2( 1:2, matches(2,:)); mat1 ];
%the Fundamental matrix
Fundamental_max = det_F_normalized_8point(pt_1,pt_2);

Fundamental_max

%Drawing the epipolar lines in first image

point_set = F1( 1:2, matches(1,:));
epiLines = epipolarLine(Fundamental_max', point_set');
%Compute the intersection points of the lines and the image border.
points = lineToBorderPoints(epiLines,size(I_1));

%plotting points on image
figure; 
imshow (I_1);
title('Epipolar lines on the first image')
hold on;
line(points(:,[1,3])',points(:,[2,4])');


%% Part 3_2 Computation of the essential matrix

Essential_mat = K_mat' * Fundamental_max * K_mat;

Essential_mat






##### SOURCE END #####
--></body></html>